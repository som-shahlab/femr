{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c81279e-a568-4e36-9906-06317accb622",
   "metadata": {},
   "source": [
    "# Train MOTOR\n",
    "\n",
    "This tutorial walks through the various steps to train a MOTOR model.\n",
    "\n",
    "Note that MOTOR requires the gpu enabled version of FEMR. See the [README](https://github.com/som-shahlab/femr#how-to-install-femr-with-cuda-support) for the relevant instructions.\n",
    "\n",
    "Training MOTOR is a three step process:\n",
    "\n",
    "- Generating a survival dictionary\n",
    "- Generating a dictionary\n",
    "- Creating batches\n",
    "- Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcdfd70-58a1-4460-80a8-db737a8c5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_7'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60ab7df-e851-44a5-ab70-7bee292be00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banned 0 out of 4523\n",
      "Got age statistics ... {\"mean\":834488.8237272066,\"std\":1516971.4691312015}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "EXTRACT_LOCATION = \"input/extract\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The first step of training CLMBR is creating a dictionary, that helps map codes to integers that can be used within a neural network.\n",
    "\"\"\"\n",
    "\n",
    "DICTIONARY_PATH = os.path.join(TARGET_DIR, \"dictionary\")\n",
    "os.system(f\"clmbr_create_dictionary {DICTIONARY_PATH} --data_path {EXTRACT_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f236f8cd-3987-47c1-b445-4b977013bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banned 0 out of 4523\n",
      "Starting to process \n",
      "RAEDY\n",
      "Got total weight 2.85142\n",
      "0 175680 332640 492480 658080 822240 999360 1244160 \n",
      "Got total weight 1024\n",
      "0 175680 332640 492480 658080 822240 999360 1244160 \n",
      "Got total weight 2.85142\n",
      "0 105120 175680 250560 332640 411840 492480 576000 658080 735840 822240 907200 999360 1110240 1244160 1441440 \n",
      "Got total weight 1024\n",
      "0 105120 175680 250560 332640 411840 492480 576000 658080 735840 822240 907200 999360 1110240 1244160 1441440 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "EXTRACT_LOCATION = \"input/extract\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The first step of training CLMBR is creating a dictionary, that helps map codes to integers that can be used within a neural network.\n",
    "\"\"\"\n",
    "\n",
    "SURVIVAL_DICTIONARY_PATH = os.path.join(TARGET_DIR, \"survival_dictionary\")\n",
    "os.system(f\"clmbr_create_survival_dictionary {SURVIVAL_DICTIONARY_PATH} --data_path {EXTRACT_LOCATION} --num_buckets 8 --size 1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89611ba9-a242-4b87-9b8f-25670d838fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clmbr_create_batches trash/tutorial_7/clmbr_batches --data_path input/extract --dictionary trash/tutorial_7/dictionary --task survival_clmbr --transformer_vocab_size 2048 --clmbr_survival_dictionary_path  trash/tutorial_7/survival_dictionary --is_hierarchical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 16:53:11,495 [MainThread  ] [INFO ]  Preparing batches with Namespace(directory='trash/tutorial_7/clmbr_batches', data_path='input/extract', dictionary_path='trash/tutorial_7/dictionary', task='survival_clmbr', transformer_vocab_size=2048, clmbr_survival_dictionary_path='trash/tutorial_7/survival_dictionary', labeled_patients_path=None, is_hierarchical=True, seed=97, val_start=80, test_start=85, batch_size=16384, note_embedding_data=None, limit_to_patients_file=None, limit_before_date=None, num_clmbr_tasks=8192)\n",
      "2023-07-21 16:53:11,535 [MainThread  ] [INFO ]  Wrote config ...\n",
      "2023-07-21 16:53:11,536 [MainThread  ] [INFO ]  Starting to load\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0When mapping codes, dropped  out of 2048\n",
      "0 out of 02048\n",
      " out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 16:53:12,293 [MainThread  ] [INFO ]  Loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 16:53:12,642 [MainThread  ] [INFO ]  Number of train patients 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The second step of training CLMBR is to prepare the batches that will actually get fed into the neural network.\n",
    "\"\"\"\n",
    "\n",
    "SURVIVAL_CLMBR_BATCHES = os.path.join(TARGET_DIR, \"clmbr_batches\")\n",
    "\n",
    "command =  f\"clmbr_create_batches {SURVIVAL_CLMBR_BATCHES} --data_path {EXTRACT_LOCATION} --dictionary {DICTIONARY_PATH} --task survival_clmbr --transformer_vocab_size 2048 --clmbr_survival_dictionary_path  {SURVIVAL_DICTIONARY_PATH} --is_hierarchical\"\n",
    "\n",
    "print(command)\n",
    "\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f654a46c-5aa7-465c-b6c5-73d8ba26ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 16:53:53,445 [MainThread  ] [INFO ]  Training model with Namespace(directory='trash/tutorial_7/survival_clmbr_model', data_path='input/extract', batches_path='trash/tutorial_7/clmbr_batches', learning_rate=0.0001, rotary_type='per_head', clmbr_survival_dim=512, num_batch_threads=3, start_from_checkpoint=None, freeze_weights=False, token_dropout=0, internal_dropout=0, weight_decay=0, max_iter=10, hidden_size=256, intermediate_size=256, n_heads=4, n_layers=1, attention_width=512, dev_batches_path=None, linear_probe=None, early_stopping_window_steps=None, with_age_beta=False)\n",
      "2023-07-21 16:53:53,464 [MainThread  ] [INFO ]  Got config {'data_path': 'input/extract', 'batch_info_path': 'trash/tutorial_7/clmbr_batches/batch_info.msgpack', 'seed': 97, 'task': {'type': 'survival_clmbr', 'num_time_bins': 8, 'num_codes': 1024, 'dim': 512, 'time_bins': (0, 175680, 332640, 492480, 658080, 822240, 999360, 1244160)}, 'transformer': {'vocab_size': 2048, 'hidden_size': 256, 'intermediate_size': 256, 'n_heads': 4, 'n_layers': 1, 'rotary': 'per_head', 'attention_width': 496, 'internal_dropout': 0, 'is_hierarchical': True, 'note_embedding_data': None, 'with_age_beta': False}, 'learning_rate': 0.0001, 'max_grad_norm': 1.0, 'weight_decay': 0, 'n_epochs': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 16:53:53,717 [MainThread  ] [INFO ]  Loaded batches 1 1\n",
      "2023-07-21 16:53:54,313 [MainThread  ] [INFO ]  Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Host Interpreter CUDA\n",
      "2023-07-21 16:53:54,315 [MainThread  ] [INFO ]  Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2023-07-21 16:53:54,315 [MainThread  ] [INFO ]  Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "2023-07-21 16:53:55,437 [MainThread  ] [INFO ]  Got dummy batch {'num_indices': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'num_patients': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'offsets': ((512,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'patient_ids': ((512,), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'task': {'event_indices': ((16384, 2), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'num_valid': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'sparse_time': [((32769,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), ((32768,), dtype('float32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), ((131072,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), ((131072,), dtype('float32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0))]}, 'transformer': {'ages': ((16384,), dtype('float32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'integer_ages': ((16384,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'label_indices': ((4096,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'length': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'normalized_ages': ((16384,), dtype('float32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'sparse_token_indices': ((4096, 2), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'valid_tokens': ((16384,), dtype('bool'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0))}}\n",
      "2023-07-21 16:53:55,900 [MainThread  ] [INFO ]  Transformed the model function\n",
      "2023-07-21 16:53:57,680 [MainThread  ] [INFO ]  Done initing {'EHRTransformer/~/SurvivalCLMBRTask': {'code_weight': ((1024, 511), dtype('float32')), 'code_weight_bias': ((1024, 1), dtype('float32'))}, 'EHRTransformer/~/SurvivalCLMBRTask/~/linear': {'b': ((4088,), dtype('float32')), 'w': ((256, 4088), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/embed': {'embeddings': ((2048, 256), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear': {'b': ((1024,), dtype('float32')), 'w': ((256, 1024), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear_1': {'b': ((256,), dtype('float32')), 'w': ((512, 256), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/rms_norm': {'scale': ((256,), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm': {'scale': ((256,), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm_1': {'scale': ((256,), dtype('float32'))}}\n",
      "2023-07-21 16:53:57,681 [MainThread  ] [INFO ]  Total params 2494456\n",
      "2023-07-21 16:53:57,681 [MainThread  ] [INFO ]  total steps 100 num train batches 1\n",
      "2023-07-21 16:53:57,682 [MainThread  ] [INFO ]  Applying decay mask {'EHRTransformer/~/SurvivalCLMBRTask': {'code_weight': True, 'code_weight_bias': False}, 'EHRTransformer/~/SurvivalCLMBRTask/~/linear': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/embed': {'embeddings': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear_1': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/rms_norm': {'scale': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm': {'scale': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm_1': {'scale': False}}\n",
      "2023-07-21 16:53:57,682 [MainThread  ] [INFO ]  Using weight decay 0\n",
      "2023-07-21 16:53:58,307 [MainThread  ] [INFO ]  Starting loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=array(0, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:00,963 [MainThread  ] [INFO ]  Starting train loss {'loss': 0.08670195192098618, 'loss2': 0.08670195192098618, 'c_statistic': -0.08670195192098618}\n",
      "2023-07-21 16:54:01,989 [MainThread  ] [INFO ]  Starting dev loss {'loss': 0.0877375677227974, 'loss2': 0.0877375677227974, 'c_statistic': -0.0877375677227974}\n",
      "Working with seed 9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 16:54:03,078 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=array(0, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:03,252 [MainThread  ] [INFO ]  Train loss {'loss': 0.08670195192098618, 'loss2': 0.08670195192098618, 'c_statistic': -0.08670195192098618}\n",
      "2023-07-21 16:54:03,274 [MainThread  ] [INFO ]  Dev loss {'loss': 0.0877375677227974, 'loss2': 0.0877375677227974, 'c_statistic': -0.0877375677227974}\n",
      "2023-07-21 16:54:03,495 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "/opt/conda/envs/femr_develop/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py:711: UserWarning: Some donated buffers were not usable: ShapedArray(uint32[16384,2]), ShapedArray(uint32[32769]), ShapedArray(float32[32768]), ShapedArray(uint32[131072]), ShapedArray(float32[131072]), ShapedArray(float32[16384]), ShapedArray(uint32[4096]), ShapedArray(int32[], weak_type=True), ShapedArray(uint32[4096,2]), ShapedArray(bool[16384]).\n",
      "See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.\n",
      "  warnings.warn(f\"Some donated buffers were not usable: {', '.join(unused_donations)}.\\n{msg}\")\n",
      "2023-07-21 16:54:08,809 [MainThread  ] [INFO ]  [Step 0]\n",
      "2023-07-21 16:54:11,635 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(2, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:12,956 [MainThread  ] [INFO ]  Train loss {'loss': 0.08670184016227722, 'loss2': 0.08670184016227722, 'c_statistic': -0.08670184016227722}\n",
      "2023-07-21 16:54:13,581 [MainThread  ] [INFO ]  Dev loss {'loss': 0.08773741126060486, 'loss2': 0.08773741126060486, 'c_statistic': -0.08773741126060486}\n",
      "2023-07-21 16:54:13,755 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-21 16:54:13,763 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(3, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:13,998 [MainThread  ] [INFO ]  Train loss {'loss': 0.08670168370008469, 'loss2': 0.08670167624950409, 'c_statistic': -0.08670168370008469}\n",
      "2023-07-21 16:54:14,012 [MainThread  ] [INFO ]  Dev loss {'loss': 0.08773712068796158, 'loss2': 0.08773712068796158, 'c_statistic': -0.08773712068796158}\n",
      "2023-07-21 16:54:14,153 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-21 16:54:14,162 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(4, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:14,326 [MainThread  ] [INFO ]  Train loss {'loss': 0.08670131862163544, 'loss2': 0.08670131117105484, 'c_statistic': -0.08670131862163544}\n",
      "2023-07-21 16:54:14,343 [MainThread  ] [INFO ]  Dev loss {'loss': 0.08773710578680038, 'loss2': 0.08773710578680038, 'c_statistic': -0.08773710578680038}\n",
      "2023-07-21 16:54:14,452 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-21 16:54:14,462 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(5, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:14,599 [MainThread  ] [INFO ]  Train loss {'loss': 0.08670095354318619, 'loss2': 0.08670095354318619, 'c_statistic': -0.08670095354318619}\n",
      "2023-07-21 16:54:14,615 [MainThread  ] [INFO ]  Dev loss {'loss': 0.08773703873157501, 'loss2': 0.08773703873157501, 'c_statistic': -0.08773703873157501}\n",
      "2023-07-21 16:54:14,716 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-21 16:54:14,724 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(6, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:14,852 [MainThread  ] [INFO ]  Train loss {'loss': 0.0867004245519638, 'loss2': 0.08670041710138321, 'c_statistic': -0.0867004245519638}\n",
      "2023-07-21 16:54:14,865 [MainThread  ] [INFO ]  Dev loss {'loss': 0.08773733675479889, 'loss2': 0.08773733675479889, 'c_statistic': -0.08773733675479889}\n",
      "2023-07-21 16:54:14,865 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-21 16:54:14,871 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(7, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:14,998 [MainThread  ] [INFO ]  Train loss {'loss': 0.08669985085725784, 'loss2': 0.08669984340667725, 'c_statistic': -0.08669985085725784}\n",
      "2023-07-21 16:54:15,012 [MainThread  ] [INFO ]  Dev loss {'loss': 0.08773729205131531, 'loss2': 0.08773729205131531, 'c_statistic': -0.08773729205131531}\n",
      "2023-07-21 16:54:15,012 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-21 16:54:15,019 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(8, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:15,147 [MainThread  ] [INFO ]  Train loss {'loss': 0.08669911324977875, 'loss2': 0.08669911324977875, 'c_statistic': -0.08669911324977875}\n",
      "2023-07-21 16:54:15,160 [MainThread  ] [INFO ]  Dev loss {'loss': 0.08773652464151382, 'loss2': 0.08773652464151382, 'c_statistic': -0.08773652464151382}\n",
      "2023-07-21 16:54:15,266 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-21 16:54:15,274 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(9, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:15,403 [MainThread  ] [INFO ]  Train loss {'loss': 0.08669842034578323, 'loss2': 0.08669842034578323, 'c_statistic': -0.08669842034578323}\n",
      "2023-07-21 16:54:15,417 [MainThread  ] [INFO ]  Dev loss {'loss': 0.08773674070835114, 'loss2': 0.08773674070835114, 'c_statistic': -0.08773674070835114}\n",
      "2023-07-21 16:54:15,417 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-21 16:54:15,423 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(10, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:15,552 [MainThread  ] [INFO ]  Train loss {'loss': 0.08669770509004593, 'loss2': 0.08669770509004593, 'c_statistic': -0.08669770509004593}\n",
      "2023-07-21 16:54:15,568 [MainThread  ] [INFO ]  Dev loss {'loss': 0.08773715049028397, 'loss2': 0.08773715049028397, 'c_statistic': -0.08773715049028397}\n",
      "2023-07-21 16:54:15,568 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-21 16:54:15,576 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(11, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-21 16:54:15,704 [MainThread  ] [INFO ]  Train loss {'loss': 0.0866965800523758, 'loss2': 0.0866965726017952, 'c_statistic': -0.0866965800523758}\n",
      "2023-07-21 16:54:15,718 [MainThread  ] [INFO ]  Dev loss {'loss': 0.08773697912693024, 'loss2': 0.08773697912693024, 'c_statistic': -0.08773697912693024}\n",
      "2023-07-21 16:54:15,719 [MainThread  ] [INFO ]  Stopping due to max iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the transformer ... (16384,) (4096,)\n",
      "Compiling the transformer ... (16384,) (4096,)\n",
      "Compiling the transformer ... (16384,) (256,)\n",
      "Compiling the transformer ... (16384,) (4096,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given the batches, it is now possible to train CLMBR. By default it will train for 100 epochs, with early stopping.\n",
    "\"\"\"\n",
    "\n",
    "MODEL_PATH = os.path.join(TARGET_DIR, \"survival_clmbr_model\")\n",
    "\n",
    "\n",
    "assert 0 == os.system(\n",
    "    f\"clmbr_train_model {MODEL_PATH} --data_path {EXTRACT_LOCATION} --batches_path {SURVIVAL_CLMBR_BATCHES} --learning_rate 1e-4 --rotary_type per_head --num_batch_threads 3 --max_iter 10 --n_layers 1 --hidden_size 256 --n_heads 4 --intermediate_size 256 --clmbr_survival_dim  512\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
