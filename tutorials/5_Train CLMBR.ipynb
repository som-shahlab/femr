{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c81279e-a568-4e36-9906-06317accb622",
   "metadata": {},
   "source": [
    "# Train CLMBR\n",
    "\n",
    "This tutorial walks through the various steps to train a CLMBR model.\n",
    "\n",
    "Note that CLMBR requires the gpu enabled version of FEMR. See the [README](https://github.com/som-shahlab/femr#how-to-install-femr-with-cuda-support) for the relevant instructions.\n",
    "\n",
    "Training CLMBR is a three step process:\n",
    "\n",
    "- Generating a dictionary\n",
    "- Creating batches\n",
    "- Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcdfd70-58a1-4460-80a8-db737a8c5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_5'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60ab7df-e851-44a5-ab70-7bee292be00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banned 0 out of 4523\n",
      "Got age statistics ... {\"mean\":834488.8237272067,\"std\":1516971.4691312027}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "EXTRACT_LOCATION = \"input/extract\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The first step of training CLMBR is creating a dictionary, that helps map codes to integers that can be used within a neural network.\n",
    "\"\"\"\n",
    "\n",
    "DICTIONARY_PATH = os.path.join(TARGET_DIR, \"dictionary\")\n",
    "os.system(f\"clmbr_create_dictionary {DICTIONARY_PATH} --data_path {EXTRACT_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89611ba9-a242-4b87-9b8f-25670d838fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 01:06:18,671 [MainThread  ] [INFO ]  Preparing batches with Namespace(directory='trash/tutorial_5/clmbr_batches', data_path='input/extract', dictionary_path='trash/tutorial_5/dictionary', task='clmbr', transformer_vocab_size=2048, clmbr_survival_dictionary_path=None, labeled_patients_path=None, is_hierarchical=False, seed=97, val_start=70, test_start=85, batch_size=16384, note_embedding_data=None, limit_to_patients_file=None, limit_before_date=None)\n",
      "2023-05-24 01:06:18,686 [MainThread  ] [INFO ]  Wrote config ...\n",
      "2023-05-24 01:06:18,686 [MainThread  ] [INFO ]  Starting to load\n",
      "2023-05-24 01:06:18,772 [MainThread  ] [INFO ]  Loaded\n",
      "2023-05-24 01:06:18,807 [MainThread  ] [INFO ]  Number of train patients 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped When mapping codes, dropped 0 out of 0 out of 20482048\n",
      "\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The second step of training CLMBR is to prepare the batches that will actually get fed into the neural network.\n",
    "\"\"\"\n",
    "\n",
    "CLMBR_BATCHES = os.path.join(TARGET_DIR, \"clmbr_batches\")\n",
    "\n",
    "os.system(\n",
    "    f\"clmbr_create_batches {CLMBR_BATCHES} --data_path {EXTRACT_LOCATION} --dictionary {DICTIONARY_PATH} --task clmbr --transformer_vocab_size 2048\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f654a46c-5aa7-465c-b6c5-73d8ba26ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 01:06:19,829 [MainThread  ] [INFO ]  Training model with Namespace(directory='trash/tutorial_5/clmbr_model', data_path='input/extract', batches_path='trash/tutorial_5/clmbr_batches', learning_rate=0.0001, rotary_type='per_head', clmbr_survival_dim=None, num_batch_threads=3, start_from_checkpoint=None, freeze_weights=False, token_dropout=0, internal_dropout=0, weight_decay=0, max_iter=10, hidden_size=256, intermediate_size=256, n_heads=4, n_layers=1, attention_width=512, dev_batches_path=None, linear_probe_head=None, early_stopping_window_steps=None)\n",
      "2023-05-24 01:06:19,840 [MainThread  ] [INFO ]  Got config {'data_path': 'input/extract', 'batch_info_path': 'trash/tutorial_5/clmbr_batches/batch_info.msgpack', 'seed': 97, 'task': {'type': 'clmbr', 'vocab_size': 8192}, 'transformer': {'vocab_size': 2048, 'hidden_size': 256, 'intermediate_size': 256, 'n_heads': 4, 'n_layers': 1, 'rotary': 'per_head', 'attention_width': 496, 'internal_dropout': 0, 'is_hierarchical': False, 'note_embedding_data': None}, 'learning_rate': 0.0001, 'max_grad_norm': 1.0, 'weight_decay': 0, 'n_epochs': 100}\n",
      "2023-05-24 01:06:19,872 [MainThread  ] [INFO ]  Loaded batches 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 01:06:20,139 [MainThread  ] [INFO ]  Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA Host\n",
      "2023-05-24 01:06:20,140 [MainThread  ] [INFO ]  Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2023-05-24 01:06:20,140 [MainThread  ] [INFO ]  Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "2023-05-24 01:06:21,606 [MainThread  ] [INFO ]  Got dummy batch {'num_indices': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'num_patients': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'offsets': ((512,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'patient_ids': ((512,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'task': {'labels': ((4096,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0))}, 'transformer': {'ages': ((16384,), dtype('float32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'label_indices': ((4096,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'length': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'normalized_ages': ((16384,), dtype('float32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'tokens': ((16384,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'valid_tokens': ((16384,), dtype('bool'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0))}}\n",
      "2023-05-24 01:06:22,038 [MainThread  ] [INFO ]  Transformed the model function\n",
      "2023-05-24 01:06:23,262 [MainThread  ] [INFO ]  Done initing {'EHRTransformer/~/CLMBRTask/~/linear': {'b': ((8192,), dtype('float32')), 'w': ((256, 8192), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/embed': {'embeddings': ((2048, 256), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear': {'b': ((1024,), dtype('float32')), 'w': ((256, 1024), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear_1': {'b': ((256,), dtype('float32')), 'w': ((512, 256), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/rms_norm': {'scale': ((256,), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm': {'scale': ((256,), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm_1': {'scale': ((256,), dtype('float32'))}}\n",
      "2023-05-24 01:06:23,263 [MainThread  ] [INFO ]  Total params 3024896\n",
      "2023-05-24 01:06:23,263 [MainThread  ] [INFO ]  total steps 100 num train batches 1\n",
      "2023-05-24 01:06:23,264 [MainThread  ] [INFO ]  Applying decay mask {'EHRTransformer/~/CLMBRTask/~/linear': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/embed': {'embeddings': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear_1': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/rms_norm': {'scale': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm': {'scale': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm_1': {'scale': False}}\n",
      "2023-05-24 01:06:23,264 [MainThread  ] [INFO ]  Using weight decay 0\n",
      "2023-05-24 01:06:23,935 [MainThread  ] [INFO ]  Starting loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=array(0, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:25,788 [MainThread  ] [INFO ]  Starting train loss {'loss': 9.358352661132812, 'c_statistic': -9.358352661132812}\n",
      "2023-05-24 01:06:26,405 [MainThread  ] [INFO ]  Starting dev loss {'loss': 9.386465072631836, 'c_statistic': -9.386465072631836}\n",
      "Working with seed 9700\n",
      "2023-05-24 01:06:26,552 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=array(0, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:26,568 [MainThread  ] [INFO ]  Train loss {'loss': 9.358352661132812, 'c_statistic': -9.358352661132812}\n",
      "2023-05-24 01:06:26,573 [MainThread  ] [INFO ]  Dev loss {'loss': 9.386465072631836, 'c_statistic': -9.386465072631836}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 01:06:26,757 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "/home/ethanid/miniconda3/envs/piton_debug/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py:711: UserWarning: Some donated buffers were not usable: ShapedArray(uint32[4096]), ShapedArray(float32[16384]), ShapedArray(uint32[4096]), ShapedArray(int32[], weak_type=True), ShapedArray(uint32[16384]), ShapedArray(bool[16384]).\n",
      "See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.\n",
      "  warnings.warn(f\"Some donated buffers were not usable: {', '.join(unused_donations)}.\\n{msg}\")\n",
      "2023-05-24 01:06:29,471 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(1, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:30,226 [MainThread  ] [INFO ]  Train loss {'loss': 9.358352661132812, 'c_statistic': -9.358352661132812}\n",
      "2023-05-24 01:06:30,656 [MainThread  ] [INFO ]  Dev loss {'loss': 9.386465072631836, 'c_statistic': -9.386465072631836}\n",
      "2023-05-24 01:06:30,692 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-24 01:06:32,092 [MainThread  ] [INFO ]  [Step 0]\n",
      "2023-05-24 01:06:32,096 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(3, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:32,108 [MainThread  ] [INFO ]  Train loss {'loss': 9.270914077758789, 'c_statistic': -9.270914077758789}\n",
      "2023-05-24 01:06:32,112 [MainThread  ] [INFO ]  Dev loss {'loss': 9.389348983764648, 'c_statistic': -9.389348983764648}\n",
      "2023-05-24 01:06:32,112 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-24 01:06:32,115 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(4, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:32,124 [MainThread  ] [INFO ]  Train loss {'loss': 9.227120399475098, 'c_statistic': -9.227120399475098}\n",
      "2023-05-24 01:06:32,127 [MainThread  ] [INFO ]  Dev loss {'loss': 9.390436172485352, 'c_statistic': -9.390436172485352}\n",
      "2023-05-24 01:06:32,128 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-24 01:06:32,130 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(5, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:32,140 [MainThread  ] [INFO ]  Train loss {'loss': 9.184638977050781, 'c_statistic': -9.184638977050781}\n",
      "2023-05-24 01:06:32,143 [MainThread  ] [INFO ]  Dev loss {'loss': 9.39208984375, 'c_statistic': -9.39208984375}\n",
      "2023-05-24 01:06:32,144 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-24 01:06:32,146 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(6, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:32,155 [MainThread  ] [INFO ]  Train loss {'loss': 9.142740249633789, 'c_statistic': -9.142740249633789}\n",
      "2023-05-24 01:06:32,159 [MainThread  ] [INFO ]  Dev loss {'loss': 9.393460273742676, 'c_statistic': -9.393460273742676}\n",
      "2023-05-24 01:06:32,159 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-24 01:06:32,162 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(7, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:32,172 [MainThread  ] [INFO ]  Train loss {'loss': 9.10057544708252, 'c_statistic': -9.10057544708252}\n",
      "2023-05-24 01:06:32,176 [MainThread  ] [INFO ]  Dev loss {'loss': 9.394940376281738, 'c_statistic': -9.394940376281738}\n",
      "2023-05-24 01:06:32,176 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-24 01:06:32,179 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(8, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:32,188 [MainThread  ] [INFO ]  Train loss {'loss': 9.059477806091309, 'c_statistic': -9.059477806091309}\n",
      "2023-05-24 01:06:32,192 [MainThread  ] [INFO ]  Dev loss {'loss': 9.396610260009766, 'c_statistic': -9.396610260009766}\n",
      "2023-05-24 01:06:32,192 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-24 01:06:32,194 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(9, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:32,204 [MainThread  ] [INFO ]  Train loss {'loss': 9.018436431884766, 'c_statistic': -9.018436431884766}\n",
      "2023-05-24 01:06:32,208 [MainThread  ] [INFO ]  Dev loss {'loss': 9.397492408752441, 'c_statistic': -9.397492408752441}\n",
      "2023-05-24 01:06:32,208 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-24 01:06:32,211 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(10, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:32,220 [MainThread  ] [INFO ]  Train loss {'loss': 8.978455543518066, 'c_statistic': -8.978455543518066}\n",
      "2023-05-24 01:06:32,223 [MainThread  ] [INFO ]  Dev loss {'loss': 9.398468971252441, 'c_statistic': -9.398468971252441}\n",
      "2023-05-24 01:06:32,223 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-05-24 01:06:32,226 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(11, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-05-24 01:06:32,235 [MainThread  ] [INFO ]  Train loss {'loss': 8.938859939575195, 'c_statistic': -8.938859939575195}\n",
      "2023-05-24 01:06:32,238 [MainThread  ] [INFO ]  Dev loss {'loss': 9.399965286254883, 'c_statistic': -9.399965286254883}\n",
      "2023-05-24 01:06:32,239 [MainThread  ] [INFO ]  Stopping due to max iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the transformer ... (16384,) (4096,)\n",
      "Compiling the transformer ... (16384,) (4096,)\n",
      "Compiling the transformer ... (16384,) (256,)\n",
      "Compiling the transformer ... (16384,) (4096,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given the batches, it is now possible to train CLMBR. By default it will train for 100 epochs, with early stopping.\n",
    "\"\"\"\n",
    "\n",
    "MODEL_PATH = os.path.join(TARGET_DIR, \"clmbr_model\")\n",
    "\n",
    "\n",
    "assert 0 == os.system(\n",
    "    f\"clmbr_train_model {MODEL_PATH} --data_path {EXTRACT_LOCATION} --batches_path {CLMBR_BATCHES} --learning_rate 1e-4 --rotary_type per_head --num_batch_threads 3 --max_iter 10 --n_layers 1 --hidden_size 256 --n_heads 4 --intermediate_size 256\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
