{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c81279e-a568-4e36-9906-06317accb622",
   "metadata": {},
   "source": [
    "# Train CLMBR\n",
    "\n",
    "This tutorial walks through the various steps to train a CLMBR model.\n",
    "\n",
    "Note that CLMBR requires the gpu enabled version of FEMR. See the [README](https://github.com/som-shahlab/femr#how-to-install-femr-with-cuda-support) for the relevant instructions.\n",
    "\n",
    "Training CLMBR is a three step process:\n",
    "\n",
    "- Generating a dictionary\n",
    "- Creating batches\n",
    "- Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcdfd70-58a1-4460-80a8-db737a8c5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_5'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60ab7df-e851-44a5-ab70-7bee292be00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banned 0 out of 4523\n",
      "Got age statistics ... {\"mean\":834488.8237272066,\"std\":1516971.4691312015}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "EXTRACT_LOCATION = \"input/extract\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The first step of training CLMBR is creating a dictionary, that helps map codes to integers that can be used within a neural network.\n",
    "\"\"\"\n",
    "\n",
    "DICTIONARY_PATH = os.path.join(TARGET_DIR, \"dictionary\")\n",
    "os.system(f\"clmbr_create_dictionary {DICTIONARY_PATH} --data_path {EXTRACT_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89611ba9-a242-4b87-9b8f-25670d838fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 12:38:59,857 [MainThread  ] [INFO ]  Preparing batches with Namespace(directory='trash/tutorial_5/clmbr_batches', data_path='input/extract', dictionary_path='trash/tutorial_5/dictionary', task='clmbr', transformer_vocab_size=2048, clmbr_survival_dictionary_path=None, labeled_patients_path=None, is_hierarchical=False, seed=97, val_start=80, test_start=85, batch_size=16384, note_embedding_data=None, limit_to_patients_file=None, limit_before_date=None, num_clmbr_tasks=8192)\n",
      "2023-07-08 12:38:59,899 [MainThread  ] [INFO ]  Wrote config ...\n",
      "2023-07-08 12:38:59,899 [MainThread  ] [INFO ]  Starting to load\n",
      "2023-07-08 12:38:59,990 [MainThread  ] [INFO ]  Loaded\n",
      "2023-07-08 12:39:00,019 [MainThread  ] [INFO ]  Number of train patients 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped When mapping codes, dropped 00 out of  out of 20482048\n",
      "\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of When mapping codes, dropped 20480 out of 2048\n",
      "\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The second step of training CLMBR is to prepare the batches that will actually get fed into the neural network.\n",
    "\"\"\"\n",
    "\n",
    "CLMBR_BATCHES = os.path.join(TARGET_DIR, \"clmbr_batches\")\n",
    "\n",
    "os.system(\n",
    "    f\"clmbr_create_batches {CLMBR_BATCHES} --data_path {EXTRACT_LOCATION} --dictionary {DICTIONARY_PATH} --task clmbr --transformer_vocab_size 2048\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f654a46c-5aa7-465c-b6c5-73d8ba26ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 12:39:01,401 [MainThread  ] [INFO ]  Training model with Namespace(directory='trash/tutorial_5/clmbr_model', data_path='input/extract', batches_path='trash/tutorial_5/clmbr_batches', learning_rate=0.0001, rotary_type='per_head', clmbr_survival_dim=None, num_batch_threads=3, start_from_checkpoint=None, freeze_weights=False, token_dropout=0, internal_dropout=0, weight_decay=0, max_iter=10, hidden_size=256, intermediate_size=256, n_heads=4, n_layers=1, attention_width=512, dev_batches_path=None, linear_probe=None, early_stopping_window_steps=None, with_age_beta=False)\n",
      "2023-07-08 12:39:01,414 [MainThread  ] [INFO ]  Got config {'data_path': 'input/extract', 'batch_info_path': 'trash/tutorial_5/clmbr_batches/batch_info.msgpack', 'seed': 97, 'task': {'type': 'clmbr', 'vocab_size': 8192}, 'transformer': {'vocab_size': 2048, 'hidden_size': 256, 'intermediate_size': 256, 'n_heads': 4, 'n_layers': 1, 'rotary': 'per_head', 'attention_width': 496, 'internal_dropout': 0, 'is_hierarchical': False, 'note_embedding_data': None, 'with_age_beta': False}, 'learning_rate': 0.0001, 'max_grad_norm': 1.0, 'weight_decay': 0, 'n_epochs': 100}\n",
      "2023-07-08 12:39:01,450 [MainThread  ] [INFO ]  Loaded batches 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 12:39:01,668 [MainThread  ] [INFO ]  Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter Host CUDA\n",
      "2023-07-08 12:39:01,668 [MainThread  ] [INFO ]  Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2023-07-08 12:39:01,669 [MainThread  ] [INFO ]  Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "2023-07-08 12:39:04,733 [MainThread  ] [INFO ]  Got dummy batch {'num_indices': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'num_patients': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'offsets': ((512,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'patient_ids': ((512,), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'task': {'labels': ((4096,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0))}, 'transformer': {'ages': ((16384,), dtype('float32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'integer_ages': ((16384,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'label_indices': ((4096,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'length': ((), dtype('int32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'normalized_ages': ((16384,), dtype('float32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'tokens': ((16384,), dtype('uint32'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)), 'valid_tokens': ((16384,), dtype('bool'), StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0))}}\n",
      "2023-07-08 12:39:05,023 [MainThread  ] [INFO ]  Transformed the model function\n",
      "2023-07-08 12:39:05,822 [MainThread  ] [INFO ]  Done initing {'EHRTransformer/~/CLMBRTask/~/linear': {'b': ((8192,), dtype('float32')), 'w': ((256, 8192), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/embed': {'embeddings': ((2048, 256), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear': {'b': ((1024,), dtype('float32')), 'w': ((256, 1024), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear_1': {'b': ((256,), dtype('float32')), 'w': ((512, 256), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/rms_norm': {'scale': ((256,), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm': {'scale': ((256,), dtype('float32'))}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm_1': {'scale': ((256,), dtype('float32'))}}\n",
      "2023-07-08 12:39:05,823 [MainThread  ] [INFO ]  Total params 3024896\n",
      "2023-07-08 12:39:05,823 [MainThread  ] [INFO ]  total steps 100 num train batches 1\n",
      "2023-07-08 12:39:05,824 [MainThread  ] [INFO ]  Applying decay mask {'EHRTransformer/~/CLMBRTask/~/linear': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/embed': {'embeddings': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear_1': {'b': False, 'w': True}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/rms_norm': {'scale': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm': {'scale': False}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/rms_norm_1': {'scale': False}}\n",
      "2023-07-08 12:39:05,824 [MainThread  ] [INFO ]  Using weight decay 0\n",
      "2023-07-08 12:39:06,226 [MainThread  ] [INFO ]  Starting loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=array(0, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:07,682 [MainThread  ] [INFO ]  Starting train loss {'loss': 9.361560821533203, 'loss2': 9.361560821533203, 'c_statistic': -9.361560821533203}\n",
      "2023-07-08 12:39:08,244 [MainThread  ] [INFO ]  Starting dev loss {'loss': 9.389105796813965, 'loss2': 9.389105796813965, 'c_statistic': -9.389105796813965}\n",
      "Working with seed 9700\n",
      "2023-07-08 12:39:08,367 [MainThread  ] [INFO ]  [Step 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n",
      "When mapping codes, dropped 0 out of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/miniconda3/envs/femr_tutorial_debug/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py:711: UserWarning: Some donated buffers were not usable: ShapedArray(uint32[4096]), ShapedArray(float32[16384]), ShapedArray(uint32[4096]), ShapedArray(int32[], weak_type=True), ShapedArray(uint32[16384]), ShapedArray(bool[16384]).\n",
      "See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.\n",
      "  warnings.warn(f\"Some donated buffers were not usable: {', '.join(unused_donations)}.\\n{msg}\")\n",
      "2023-07-08 12:39:10,975 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(1, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:11,653 [MainThread  ] [INFO ]  Train loss {'loss': 9.361560821533203, 'loss2': 9.361560821533203, 'c_statistic': -9.361560821533203}\n",
      "2023-07-08 12:39:12,080 [MainThread  ] [INFO ]  Dev loss {'loss': 9.389105796813965, 'loss2': 9.389105796813965, 'c_statistic': -9.389105796813965}\n",
      "2023-07-08 12:39:12,231 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-08 12:39:13,582 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(2, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:13,594 [MainThread  ] [INFO ]  Train loss {'loss': 9.361593246459961, 'loss2': 9.361593246459961, 'c_statistic': -9.361593246459961}\n",
      "2023-07-08 12:39:13,599 [MainThread  ] [INFO ]  Dev loss {'loss': 9.389192581176758, 'loss2': 9.389192581176758, 'c_statistic': -9.389192581176758}\n",
      "2023-07-08 12:39:13,599 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-08 12:39:13,604 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(3, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:13,614 [MainThread  ] [INFO ]  Train loss {'loss': 9.361457824707031, 'loss2': 9.361457824707031, 'c_statistic': -9.361457824707031}\n",
      "2023-07-08 12:39:13,618 [MainThread  ] [INFO ]  Dev loss {'loss': 9.389452934265137, 'loss2': 9.389452934265137, 'c_statistic': -9.389452934265137}\n",
      "2023-07-08 12:39:13,618 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-08 12:39:13,622 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(4, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:13,632 [MainThread  ] [INFO ]  Train loss {'loss': 9.36137866973877, 'loss2': 9.36137866973877, 'c_statistic': -9.36137866973877}\n",
      "2023-07-08 12:39:13,637 [MainThread  ] [INFO ]  Dev loss {'loss': 9.389192581176758, 'loss2': 9.389192581176758, 'c_statistic': -9.389192581176758}\n",
      "2023-07-08 12:39:13,637 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-08 12:39:13,641 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(5, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:13,651 [MainThread  ] [INFO ]  Train loss {'loss': 9.361272811889648, 'loss2': 9.361272811889648, 'c_statistic': -9.361272811889648}\n",
      "2023-07-08 12:39:13,656 [MainThread  ] [INFO ]  Dev loss {'loss': 9.389105796813965, 'loss2': 9.389105796813965, 'c_statistic': -9.389105796813965}\n",
      "2023-07-08 12:39:13,656 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-08 12:39:13,659 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(6, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:13,669 [MainThread  ] [INFO ]  Train loss {'loss': 9.360976219177246, 'loss2': 9.360976219177246, 'c_statistic': -9.360976219177246}\n",
      "2023-07-08 12:39:13,674 [MainThread  ] [INFO ]  Dev loss {'loss': 9.389019012451172, 'loss2': 9.389019012451172, 'c_statistic': -9.389019012451172}\n",
      "2023-07-08 12:39:13,794 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-08 12:39:13,799 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(7, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:13,810 [MainThread  ] [INFO ]  Train loss {'loss': 9.360629081726074, 'loss2': 9.360629081726074, 'c_statistic': -9.360629081726074}\n",
      "2023-07-08 12:39:13,815 [MainThread  ] [INFO ]  Dev loss {'loss': 9.389105796813965, 'loss2': 9.389105796813965, 'c_statistic': -9.389105796813965}\n",
      "2023-07-08 12:39:13,815 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-08 12:39:13,820 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(8, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:13,831 [MainThread  ] [INFO ]  Train loss {'loss': 9.360361099243164, 'loss2': 9.360361099243164, 'c_statistic': -9.360361099243164}\n",
      "2023-07-08 12:39:13,835 [MainThread  ] [INFO ]  Dev loss {'loss': 9.388845443725586, 'loss2': 9.388845443725586, 'c_statistic': -9.388845443725586}\n",
      "2023-07-08 12:39:13,923 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-08 12:39:13,928 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(9, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:13,938 [MainThread  ] [INFO ]  Train loss {'loss': 9.360023498535156, 'loss2': 9.360023498535156, 'c_statistic': -9.360023498535156}\n",
      "2023-07-08 12:39:13,943 [MainThread  ] [INFO ]  Dev loss {'loss': 9.388845443725586, 'loss2': 9.388845443725586, 'c_statistic': -9.388845443725586}\n",
      "2023-07-08 12:39:13,943 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-08 12:39:13,946 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(10, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:13,957 [MainThread  ] [INFO ]  Train loss {'loss': 9.359622955322266, 'loss2': 9.359622955322266, 'c_statistic': -9.359622955322266}\n",
      "2023-07-08 12:39:13,961 [MainThread  ] [INFO ]  Dev loss {'loss': 9.389019012451172, 'loss2': 9.389019012451172, 'c_statistic': -9.389019012451172}\n",
      "2023-07-08 12:39:13,962 [MainThread  ] [INFO ]  Continuing to train ...\n",
      "2023-07-08 12:39:13,966 [MainThread  ] [INFO ]  Loss scale DynamicLossScale(loss_scale=Array(32768., dtype=float32), counter=Array(11, dtype=int32), period=2000, factor=2, min_loss_scale=array(1., dtype=float32))\n",
      "2023-07-08 12:39:13,976 [MainThread  ] [INFO ]  Train loss {'loss': 9.359251022338867, 'loss2': 9.359251022338867, 'c_statistic': -9.359251022338867}\n",
      "2023-07-08 12:39:13,980 [MainThread  ] [INFO ]  Dev loss {'loss': 9.388758659362793, 'loss2': 9.388758659362793, 'c_statistic': -9.388758659362793}\n",
      "2023-07-08 12:39:14,051 [MainThread  ] [INFO ]  Stopping due to max iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the transformer ... (16384,) (4096,)\n",
      "Compiling the transformer ... (16384,) (4096,)\n",
      "Compiling the transformer ... (16384,) (256,)\n",
      "Compiling the transformer ... (16384,) (4096,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given the batches, it is now possible to train CLMBR. By default it will train for 100 epochs, with early stopping.\n",
    "\"\"\"\n",
    "\n",
    "MODEL_PATH = os.path.join(TARGET_DIR, \"clmbr_model\")\n",
    "\n",
    "\n",
    "assert 0 == os.system(\n",
    "    f\"clmbr_train_model {MODEL_PATH} --data_path {EXTRACT_LOCATION} --batches_path {CLMBR_BATCHES} --learning_rate 1e-4 --rotary_type per_head --num_batch_threads 3 --max_iter 10 --n_layers 1 --hidden_size 256 --n_heads 4 --intermediate_size 256\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
