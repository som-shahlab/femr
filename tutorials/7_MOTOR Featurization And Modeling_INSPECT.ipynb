{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c59464-ef8f-4fb9-a0a6-ebb68475e2a9",
   "metadata": {},
   "source": [
    "# Using CLMBR to generate features and training models on those features\n",
    "\n",
    "We can use a trained CLMBR model to generate features and then use those features in a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe93d59d-f135-46f6-b0a7-2d75d9b18e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_7_INSPECT'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)\n",
    "\n",
    "\n",
    "num_proc = 20"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f2f732a",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "def validateJSON(jsonData):\n",
    "    try:\n",
    "        json.loads(jsonData)\n",
    "    except ValueError as err:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def rectifyJSON(jsonFile):\n",
    "    new_json = open(jsonFile.replace('.json', '_new.json'), 'w')\n",
    "    with open(jsonFile, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.replace(\"'\", '\"')\n",
    "            new_json.write(line)\n",
    "    new_json.close()\n",
    "    return jsonFile.replace('.json', '_new.json')\n",
    "\n",
    "\n",
    "# pretrain_model_config = '/share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model/config.json'\n",
    "# pretrain_model_config = '/share/pi/nigam/ethanid/femrv3/femr/tutorials/trash/tutorial_6/motor_model/config.json'\n",
    "\n",
    "if not validateJSON(pretrain_model_config):\n",
    "    print('Rectifying JSON')\n",
    "    pretrain_model_config = rectifyJSON(pretrain_model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d741a7-46a2-4760-a369-3efb01afd804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/nigam/projects/zphuo/repos/transformers/src/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffb6416a27c442c97b8b217ec158ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1916 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model were not used when initializing FEMRModel: ['task_model.final_layer.bias', 'task_model.final_layer.weight', 'task_model.task_layer.bias', 'task_model.task_layer.weight']\n",
      "- This IS expected if you are initializing FEMRModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FEMRModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82d0c35aedd4828b58e00aaded291cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches 1252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8da34c1a73c40ab827e67104cee3c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_ids (2060,)\n",
      "feature_times (2060,)\n",
      "features (2060, 64)\n"
     ]
    }
   ],
   "source": [
    "import femr.models.transformer\n",
    "import pyarrow.csv\n",
    "import datasets\n",
    "import pickle\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# First, we compute our features\n",
    "\n",
    "# Load some labels\n",
    "# labels = pyarrow.csv.read_csv('input/labels.csv').to_pylist()\n",
    "# label_csv = '/share/pi/nigam/projects/zphuo/data/PE/inspect/cohort_0.2.0_master_file_anon.csv'\n",
    "label_csv_subset = '/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds/cohort_0.2.0_master_file_anon_subset.csv'\n",
    "\n",
    "labels_table = pyarrow.csv.read_csv(label_csv_subset)\n",
    "\n",
    "# filter out censored\n",
    "selected_table = labels_table.select(['patient_id', 'procedure_time', '12_month_PH'])\n",
    "filtered_table = selected_table.filter(pa.compute.field(\"12_month_PH\") != \"Censored\")\n",
    "\n",
    "# cast to bool\n",
    "casted_column = pc.cast(filtered_table.column('12_month_PH'), target_type=pa.bool_())\n",
    "filtered_table = filtered_table.set_column(filtered_table.schema.get_field_index('12_month_PH'), pa.field('12_month_PH', pa.bool_()), casted_column)\n",
    "\n",
    "\n",
    "columns = {name: filtered_table.column(name) for name in filtered_table.column_names}\n",
    "columns['prediction_time'] = columns.pop('procedure_time')\n",
    "columns['boolean_value'] = columns.pop('12_month_PH')\n",
    "filtered_table = pa.Table.from_arrays(list(columns.values()), names=list(columns.keys()))\n",
    "\n",
    "labels = filtered_table.to_pylist()\n",
    "\n",
    "# Load our data\n",
    "# dataset = datasets.Dataset.from_parquet(\"input/meds/data/*\")\n",
    "parquet_folder = '/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds/data_subset/*'\n",
    "dataset = datasets.Dataset.from_parquet(parquet_folder)\n",
    "\n",
    "# femr.ontology.Ontology will create one given an athena path and code_metadata\n",
    "# We need an ontology for MOTOR\n",
    "with open('input/meds/ontology.pkl', 'rb') as f:\n",
    "    ontology = pickle.load(f)\n",
    "\n",
    "model_folder = '/share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model'\n",
    "# model_folder = '/share/pi/nigam/ethanid/femrv3/femr/tutorials/trash/tutorial_6/motor_model'\n",
    "\n",
    "\n",
    "features = femr.models.transformer.compute_features(dataset, model_folder, labels, num_proc=num_proc, tokens_per_batch=128, ontology=ontology)\n",
    "\n",
    "# We have our features\n",
    "for k, v in features.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c75a9",
   "metadata": {},
   "source": [
    "# Joining features and labels\n",
    "\n",
    "Given a feature set, it's important to be able to join a set of labels to those features.\n",
    "\n",
    "This can be done with femr.featurizers.join_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad882f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Missing features for label {'patient_id': 137994065, 'prediction_time': datetime.datetime(2221, 9, 26, 19, 30), 'boolean_value': False}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfemr\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeaturizers\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m features_and_labels \u001b[39m=\u001b[39m femr\u001b[39m.\u001b[39;49mfeaturizers\u001b[39m.\u001b[39;49mjoin_labels(features, labels)\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m features_and_labels\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(k, v\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/femr/src/femr/featurizers/core.py:343\u001b[0m, in \u001b[0;36mjoin_labels\u001b[0;34m(features, labels)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m label_index \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(labels):\n\u001b[1;32m    341\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[39massert\u001b[39;00m patient_id \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m labels[label_index][\u001b[39m\"\u001b[39m\u001b[39mpatient_id\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing features for label \u001b[39m\u001b[39m{\u001b[39;00mlabels[label_index]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m \u001b[39mif\u001b[39;00m patient_id \u001b[39m<\u001b[39m labels[label_index][\u001b[39m\"\u001b[39m\u001b[39mpatient_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    345\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Missing features for label {'patient_id': 137994065, 'prediction_time': datetime.datetime(2221, 9, 26, 19, 30), 'boolean_value': False}"
     ]
    }
   ],
   "source": [
    "import femr.featurizers\n",
    "\n",
    "features_and_labels = femr.featurizers.join_labels(features, labels)\n",
    "\n",
    "for k, v in features_and_labels.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192ccc8",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "When using a pretrained CLMBR model, we have to be very careful to use the splits used for the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c49417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import femr.splits\n",
    "import numpy as np\n",
    "\n",
    "# We split into a global training and test set\n",
    "split = femr.splits.PatientSplit.load_from_csv('input/motor_model/main_split.csv')\n",
    "\n",
    "train_mask = np.isin(features_and_labels['patient_ids'], split.train_patient_ids)\n",
    "test_mask = np.isin(features_and_labels['patient_ids'], split.test_patient_ids)\n",
    "\n",
    "percent_train = .70\n",
    "X_train, y_train = (\n",
    "    features_and_labels['features'][train_mask],\n",
    "    features_and_labels['boolean_values'][train_mask],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    features_and_labels['features'][test_mask],\n",
    "    features_and_labels['boolean_values'][test_mask],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deca785",
   "metadata": {},
   "source": [
    "# Building Models\n",
    "\n",
    "The generated features can then be used to build your standard models. In this case we construct both logistic regression and XGBoost models and evaluate them.\n",
    "\n",
    "Performance is perfect since our task (predicting gender) is 100% determined by the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5ad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Logistic Regression ----\n",
      "Train:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n",
      "Test:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def run_analysis(title: str, y_train, y_train_proba, y_test, y_test_proba):\n",
    "    print(f\"---- {title} ----\")\n",
    "    print(\"Train:\")\n",
    "    print_metrics(y_train, y_train_proba)\n",
    "    print(\"Test:\")\n",
    "    print_metrics(y_test, y_test_proba)\n",
    "\n",
    "def print_metrics(y_true, y_proba):\n",
    "    y_pred = y_proba > 0.5\n",
    "    auroc = sklearn.metrics.roc_auc_score(y_true, y_proba)\n",
    "    aps = sklearn.metrics.average_precision_score(y_true, y_proba)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    print(\"\\tAUROC:\", auroc)\n",
    "    print(\"\\tAPS:\", aps)\n",
    "    print(\"\\tAccuracy:\", accuracy)\n",
    "    print(\"\\tF1 Score:\", f1)\n",
    "\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegressionCV(penalty=\"l2\", solver=\"liblinear\").fit(X_train, y_train)\n",
    "y_train_proba = model.predict_proba(X_train)[::, 1]\n",
    "y_test_proba = model.predict_proba(X_test)[::, 1]\n",
    "run_analysis(\"Logistic Regression\", y_train, y_train_proba, y_test, y_test_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd210c0e8761410aac1aa73898a7228901cf13f71a476a6a00dddfdd82855066"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
