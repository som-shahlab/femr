{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf343c1c-ad8d-4fdb-a142-c501e579e288",
   "metadata": {},
   "source": [
    "# Count Featurization And Models\n",
    "\n",
    "FEMR contains several utilities to implement common featurization strategies.\n",
    "\n",
    "[CountFeaturizer](https://github.com/som-shahlab/femr/blob/main/src/femr/featurizers/featurizers.py#L180) is the main class and it documents the various supported options.\n",
    "\n",
    "In order to use the featurizer, you must construct a featurize list, prepare the featurizers, and then featurize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892ab2d5-0c5a-43c9-a210-9201f775e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import femr.featurizers\n",
    "\n",
    "# Load some labels\n",
    "with open('input/labels.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "    \n",
    "# Define our featurizer\n",
    "\n",
    "# Note that we are using both ages and counts here\n",
    "age = femr.featurizers.AgeFeaturizer(is_normalize=False)\n",
    "count = femr.featurizers.CountFeaturizer(string_value_combination=True)\n",
    "featurizer_age_count = femr.featurizers.FeaturizerList([age, count])\n",
    "\n",
    "# Preprocessing the featurizers, which includes processes such as normalizing age.\n",
    "featurizer_age_count.preprocess_featurizers(\"input/extract\", labels)\n",
    "\n",
    "# Actually do the featurization\n",
    "results = featurizer_age_count.featurize(\"input/extract\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dcc2d3f-24f5-4614-b7c7-72b72d3edf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t20.013699\n",
      "  (0, 1)\t1.0\n",
      "  (0, 3376)\t1.0 3 False 1990-01-07T00:00:00.000000\n"
     ]
    }
   ],
   "source": [
    "# Results consist of four components, the feature matrix, the patient ids, the label values, and the prediction times\n",
    "\n",
    "features, patient_ids, label_values, prediction_times = results\n",
    "\n",
    "print(features[0,:], patient_ids[0], label_values[0], prediction_times[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66934476-c40a-467c-8702-b0d7021d92bf",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "FEMR contains utilities for doing hash based patient splitted, where splits are determined based on a hash value of the patient id.\n",
    "\n",
    "This is a deterministic approximate approach for splitting that has the advantage of stability and scalability.\n",
    "\n",
    "database.compute_split(seed, pid) return as a psuedo-random number between 0 and 99 (inclusive) to help construct splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01acd922-668b-481b-8dbb-54ab6ae433af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import femr.datasets\n",
    "import numpy as np\n",
    "\n",
    "database = femr.datasets.PatientDatabase(\"input/extract\")\n",
    "\n",
    "percent_train = .70\n",
    "split_seed = 97\n",
    "\n",
    "hashed_pids = np.array([database.compute_split(split_seed, pid) for pid in patient_ids])\n",
    "train_pids_idx = np.where(hashed_pids < (percent_train * 100))[0]\n",
    "test_pids_idx = np.where(hashed_pids >= (percent_train * 100))[0]\n",
    "\n",
    "X_train, y_train = (\n",
    "    features[train_pids_idx],\n",
    "    label_values[train_pids_idx],\n",
    ")\n",
    "X_test, y_test = features[test_pids_idx], label_values[test_pids_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaeb7e5-eb48-46f5-ae59-9abfbc0dcef5",
   "metadata": {},
   "source": [
    "# Building Models\n",
    "\n",
    "The generated features can then be used to build your standard models. In this case we construct both logistic regression and XGBoost models and evaluate them.\n",
    "\n",
    "Performance is perfect since our task (predicting gender) is 100% determined by the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caae3126-1437-408e-b25f-04568e15c96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Logistic Regression ----\n",
      "Train:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n",
      "Test:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n",
      "---- XGBoost ----\n",
      "Train:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n",
      "Test:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def run_analysis(title: str, y_train, y_train_proba, y_test, y_test_proba):\n",
    "    print(f\"---- {title} ----\")\n",
    "    print(\"Train:\")\n",
    "    print_metrics(y_train, y_train_proba)\n",
    "    print(\"Test:\")\n",
    "    print_metrics(y_test, y_test_proba)\n",
    "\n",
    "def print_metrics(y_true, y_proba):\n",
    "    y_pred = y_proba > 0.5\n",
    "    auroc = sklearn.metrics.roc_auc_score(y_true, y_proba)\n",
    "    aps = sklearn.metrics.average_precision_score(y_true, y_proba)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    print(\"\\tAUROC:\", auroc)\n",
    "    print(\"\\tAPS:\", aps)\n",
    "    print(\"\\tAccuracy:\", accuracy)\n",
    "    print(\"\\tF1 Score:\", f1)\n",
    "\n",
    "\n",
    "scaler = sklearn.preprocessing.MaxAbsScaler().fit(\n",
    "    X_train\n",
    ")  # best for sparse data: see https://scikit-learn.org/stable/modules/preprocessing.html#scaling-sparse-data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "model = sklearn.linear_model.LogisticRegressionCV(penalty=\"l2\", solver=\"liblinear\").fit(X_train_scaled, y_train)\n",
    "y_train_proba = model.predict_proba(X_train_scaled)[::, 1]\n",
    "y_test_proba = model.predict_proba(X_test_scaled)[::, 1]\n",
    "run_analysis(\"Logistic Regression\", y_train, y_train_proba, y_test, y_test_proba)\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_train_proba = model.predict_proba(X_train)[::, 1]\n",
    "y_test_proba = model.predict_proba(X_test)[::, 1]\n",
    "run_analysis(\"XGBoost\", y_train, y_train_proba, y_test, y_test_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
